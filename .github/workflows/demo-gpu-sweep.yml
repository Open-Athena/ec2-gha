name: Demo â€“ GPU instance sweep
on:
  workflow_dispatch:
  workflow_call:
permissions:
  id-token: write  # Required for AWS OIDC authentication
  contents: read   # Required for actions/checkout
jobs:
  # Launch GPU instances with PyTorch 2.7 DLAMI
  g4dn:
    name: g4dn.xlarge
    uses: ./.github/workflows/runner.yml
    with:
      ec2_instance_type: g4dn.xlarge
      ec2_image_id: ami-0f20cc6143e3cdb84  # PyTorch 2.7 Ubuntu 22.04
      instance_name: "gpu-sweep/g4dn#$run_number"
    secrets: inherit
  g5:
    name: g5.xlarge
    uses: ./.github/workflows/runner.yml
    with:
      ec2_instance_type: g5.xlarge
      ec2_image_id: ami-0f20cc6143e3cdb84  # PyTorch 2.7 Ubuntu 22.04
      instance_name: "gpu-sweep/g5#$run_number"
    secrets: inherit
  g6:
    name: g6.xlarge
    uses: ./.github/workflows/runner.yml
    with:
      ec2_instance_type: g6.xlarge
      ec2_image_id: ami-0f20cc6143e3cdb84  # PyTorch 2.7 Ubuntu 22.04
      instance_name: "gpu-sweep/g6#$run_number"
    secrets: inherit
  g5g:
    name: g5g.xlarge
    uses: ./.github/workflows/runner.yml
    with:
      ec2_instance_type: g5g.xlarge
      ec2_image_id: ami-030b738fa2282338b  # PyTorch 2.7 Ubuntu 22.04 ARM64
      instance_name: "gpu-sweep/g5g#$run_number"
    secrets: inherit

  # Test jobs for each GPU instance
  test-g4dn:
    name: Test g4dn.xlarge
    needs: g4dn
    runs-on: ${{ needs.g4dn.outputs.id }}
    steps:
      - name: GPU Test
        run: |
          nvidia-smi | grep Tesla
          python3 -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
          python3 -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"

  test-g5:
    name: Test g5.xlarge
    needs: g5
    runs-on: ${{ needs.g5.outputs.id }}
    steps:
      - name: GPU Test
        run: |
          nvidia-smi | grep "NVIDIA A10"
          python3 -c "import torch; print(f'PyTorch: {torch.__version__}, GPU: {torch.cuda.get_device_name(0)}')"

  test-g6:
    name: Test g6.xlarge
    needs: g6
    runs-on: ${{ needs.g6.outputs.id }}
    steps:
      - name: GPU Test
        run: |
          nvidia-smi
          python3 -c "import torch; print(f'PyTorch: {torch.__version__}, GPU: {torch.cuda.get_device_name(0)}')"

  test-g5g:
    name: Test g5g.xlarge
    needs: g5g
    runs-on: ${{ needs.g5g.outputs.id }}
    steps:
      - name: GPU Info
        run: |
          echo "=== GPU Instance Information ==="
          echo "g5g.xlarge: AWS Graviton (ARM64) + NVIDIA T4g GPU"
          nvidia-smi
          echo ""
          echo "=== PyTorch Version ==="
          python3 -c "import torch; print(f'PyTorch: {torch.__version__}')"
          python3 -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
          python3 -c "import torch; print(f'CUDA Version: {torch.version.cuda}')"
          python3 -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
      - name: Basic GPU Test
        run: |
          python3 -c "
          import torch
          device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
          print(f'Using device: {device}')
          if device.type == 'cuda':
              x = torch.randn(1000, 1000).to(device)
              y = torch.randn(1000, 1000).to(device)
              z = torch.matmul(x, y)
              print(f'Matrix multiplication result shape: {z.shape}')
          "
