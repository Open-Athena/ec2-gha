#!/bin/bash
set -e

# Enable debug tracing to a file for troubleshooting
exec 2> >(tee -a /var/log/runner-debug.log >&2)

# Set debug variable for use in shared functions
debug="$debug"

# Conditionally enable debug mode
[ "$debug" = "true" ] || [ "$debug" = "True" ] || [ "$debug" = "1" ] && set -x

# Determine home directory early since it's needed by shared functions
homedir="$homedir"
if [ -z "$$homedir" ] || [ "$$homedir" = "AUTO" ]; then
  # Try to find the default non-root user's home directory
  for user in ubuntu ec2-user centos admin debian fedora alpine arch; do
    if id "$$user" &>/dev/null; then
      homedir="/home/$$user"
      echo "[$$(date '+%Y-%m-%d %H:%M:%S')] Auto-detected homedir: $$homedir" | tee -a /var/log/runner-setup.log
      break
    fi
  done

  # Fallback if no standard user found
  if [ -z "$$homedir" ] || [ "$$homedir" = "AUTO" ]; then
    homedir=$$(getent passwd | awk -F: '$$3 >= 1000 && $$3 < 65534 && $$6 ~ /^\\/home\\// {print $$6}' | while read dir; do
      if [ -d "$$dir" ]; then
        echo "$$dir"
        break
      fi
    done)
    if [ -z "$$homedir" ]; then
      homedir="/home/ec2-user"  # Ultimate fallback
      echo "[$$(date '+%Y-%m-%d %H:%M:%S')] Using fallback homedir: $$homedir" | tee -a /var/log/runner-setup.log
    else
      owner=$$(stat -c "%U" "$$homedir" 2>/dev/null || stat -f "%Su" "$$homedir" 2>/dev/null)
      echo "[$$(date '+%Y-%m-%d %H:%M:%S')] Detected homedir: $$homedir (owner: $$owner)" | tee -a /var/log/runner-setup.log
    fi
  fi
else
  echo "[$$(date '+%Y-%m-%d %H:%M:%S')] Using specified homedir: $$homedir" | tee -a /var/log/runner-setup.log
fi
export homedir

# Set common paths
B=/usr/local/bin
V=/var/run/github-runner

# Write shared functions that will be used by multiple scripts
# First write the variables that need template expansion
cat > $$B/runner-common.sh << EOSF
# Auto-generated shared functions and variables
# Set homedir for scripts that source this file
homedir="$$homedir"
debug="$$debug"
export homedir debug

EOSF

# Then append the shared functions using a quoted here-doc to prevent variable expansion
cat >> $$B/runner-common.sh << 'EOSF'
$shared_functions
EOSF

chmod +x $$B/runner-common.sh
source $$B/runner-common.sh

logger "EC2-GHA: Starting userdata script"
trap 'logger "EC2-GHA: Script failed at line $$LINENO with exit code $$?"' ERR
trap 'terminate_instance "Setup script failed with error on line $$LINENO"' ERR
# Handle watchdog termination signal
trap 'if [ -f $$V-watchdog-terminate ]; then terminate_instance "No runners registered within timeout"; else terminate_instance "Script terminated"; fi' TERM

# Set up registration timeout failsafe - terminate if runner doesn't register in time
REGISTRATION_TIMEOUT="$runner_registration_timeout"
# Validate timeout is a number, default to 300 if not
if ! [[ "$$REGISTRATION_TIMEOUT" =~ ^[0-9]+$$ ]]; then
  logger "EC2-GHA: Invalid timeout '$$REGISTRATION_TIMEOUT', using default 300"
  REGISTRATION_TIMEOUT=300
fi
logger "EC2-GHA: Registration timeout set to $$REGISTRATION_TIMEOUT seconds"
# Create a marker file for watchdog termination request
touch $$V-watchdog-active
(
  log "Watchdog: Starting $$REGISTRATION_TIMEOUT second timeout"
  sleep $$REGISTRATION_TIMEOUT
  if [ ! -f $$V-registered ]; then
    log "Watchdog: Registration marker not found after timeout"
    # Signal main process to terminate instead of doing it directly
    touch $$V-watchdog-terminate
    # Kill the main script process to trigger its ERR trap
    kill -TERM $$$$ 2>/dev/null || true
  else
    log "Watchdog: Registration marker found, exiting normally"
  fi
  rm -f $$V-watchdog-active
) &
REGISTRATION_WATCHDOG_PID=$$!
log "Watchdog PID: $$REGISTRATION_WATCHDOG_PID"
echo $$REGISTRATION_WATCHDOG_PID > $$V-watchdog.pid

# Run any custom user data script provided by the user
$userdata

exec >> /var/log/runner-setup.log 2>&1
log "Starting runner setup"

# Fetch instance metadata for labeling and logging
INSTANCE_TYPE=$$(get_metadata "instance-type")
INSTANCE_ID=$$(get_metadata "instance-id")
REGION=$$(get_metadata "placement/region")
AZ=$$(get_metadata "placement/availability-zone")
log "Instance metadata: Type=$${INSTANCE_TYPE} ID=$${INSTANCE_ID} Region=$${REGION} AZ=$${AZ}"

# Set up maximum lifetime timeout - instance will terminate after this time regardless of job status
MAX_LIFETIME_MINUTES=$max_instance_lifetime
log "Setting up maximum lifetime timeout: $${MAX_LIFETIME_MINUTES} minutes"
nohup bash -c "sleep $${MAX_LIFETIME_MINUTES}m && echo '[$$(date)] Maximum lifetime reached' && shutdown -h now" > /var/log/max-lifetime.log 2>&1 &

# Configure CloudWatch Logs if a log group is specified
if [ "$cloudwatch_logs_group" != "" ]; then
  log "Installing CloudWatch agent"
  # Use a subshell to prevent CloudWatch failures from stopping the entire script
  (
    # Detect package manager and install CloudWatch agent
    if command -v dpkg >/dev/null 2>&1; then
      # Debian/Ubuntu
      log "Detected dpkg-based system"
      wait_for_dpkg_lock
      wget -q https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb
      dpkg -i -E ./amazon-cloudwatch-agent.deb
      rm amazon-cloudwatch-agent.deb
    elif command -v rpm >/dev/null 2>&1; then
      # RHEL/CentOS/Amazon Linux
      log "Detected rpm-based system"
      wget -q https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm
      rpm -U ./amazon-cloudwatch-agent.rpm
      rm amazon-cloudwatch-agent.rpm
    else
      log "WARNING: Unable to detect package manager, skipping CloudWatch agent installation"
    fi
    # Build CloudWatch config with factored strings
    P='"file_path":'
    G=',"log_group_name":"$cloudwatch_logs_group","log_stream_name":"{instance_id}/'
    Z='","timezone":"UTC"}'
    H="$$homedir"
    cat > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json << EOF
{"agent":{"run_as_user":"cwagent"},"logs":{"logs_collected":{"files":{"collect_list":[
{$$P"/var/log/runner-setup.log"$${G}runner-setup$$Z,
{$$P"/var/log/runner-debug.log"$${G}runner-debug$$Z,
{$$P"/tmp/job-started-hook.log"$${G}job-started$$Z,
{$$P"/tmp/job-completed-hook.log"$${G}job-completed$$Z,
{$$P"/tmp/termination-check.log"$${G}termination$$Z,
{$$P"/tmp/runner-*-config.log"$${G}runner-config$$Z,
{$$P"$$H/_diag/Runner_**.log"$${G}runner-diag$$Z,
{$$P"$$H/_diag/Worker_**.log"$${G}worker-diag$$Z
]}}}}
EOF
    /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json -s
    log "CloudWatch agent started"
  ) || log "WARNING: CloudWatch agent installation failed, continuing without it"
fi

# Configure SSH access if public key provided (useful for debugging)
if [ -n "$ssh_pubkey" ]; then
  log "Configuring SSH access"
  # Determine the default user based on the home directory owner
  DEFAULT_USER=$$(stat -c "%U" "$$homedir" 2>/dev/null || echo "root")
  mkdir -p "$$homedir/.ssh"
  chmod 700 "$$homedir/.ssh"
  echo "$ssh_pubkey" >> "$$homedir/.ssh/authorized_keys"
  chmod 600 "$$homedir/.ssh/authorized_keys"
  if [ "$$DEFAULT_USER" != "root" ]; then
    chown -R "$$DEFAULT_USER:$$DEFAULT_USER" "$$homedir/.ssh"
  fi
  log "SSH key added for user $$DEFAULT_USER"
fi

log "Working directory: $$homedir"
cd "$$homedir"

# Run any pre-runner script provided by the user
echo "$script" > pre-runner-script.sh
log "Running pre-runner script"
source pre-runner-script.sh
export RUNNER_ALLOW_RUNASROOT=1

# Number of runners to configure on this instance
RUNNERS_PER_INSTANCE=$runners_per_instance

# Download GitHub Actions runner binary
ARCH=$$(uname -m)
if [ "$$ARCH" = "aarch64" ] || [ "$$ARCH" = "arm64" ]; then
  RUNNER_URL=$$(echo "$runner_release" | sed 's/x64/arm64/g')
  log "ARM detected, using: $$RUNNER_URL"
else
  RUNNER_URL="$runner_release"
  log "x64 detected, using: $$RUNNER_URL"
fi

if command -v curl >/dev/null 2>&1; then
  curl -L $$RUNNER_URL -o /tmp/runner.tar.gz
elif command -v wget >/dev/null 2>&1; then
  wget -q $$RUNNER_URL -O /tmp/runner.tar.gz
else
  log_error "Neither curl nor wget found. Cannot download runner."
  terminate_instance "No download tool available"
fi
log "Downloaded runner binary"

# Create job tracking scripts - these are called by GitHub runner hooks
# job-started-hook.sh is called when a job starts
cat > $$B/job-started-hook.sh << 'EOFS'
#!/bin/bash
exec >> /tmp/job-started-hook.log 2>&1
V="/var/run/github-runner"
RUNNER_IDX="$${RUNNER_INDEX:-0}"
echo "[$$(date)] Runner-$$RUNNER_IDX: ${log_prefix_job_started} $${GITHUB_JOB}"
mkdir -p $$V-jobs
echo '{\"status\":\"running\",\"runner\":\"'$$RUNNER_IDX'\"}' > $$V-jobs/$${GITHUB_RUN_ID}-$${GITHUB_JOB}-$$RUNNER_IDX.job
touch $$V-last-activity $$V-has-run-job
EOFS

# job-completed-hook.sh is called when a job completes
cat > $$B/job-completed-hook.sh << 'EOFC'
#!/bin/bash
exec >> /tmp/job-completed-hook.log 2>&1
V="/var/run/github-runner"
RUNNER_IDX="$${RUNNER_INDEX:-0}"
echo "[$$(date)] Runner-$$RUNNER_IDX: ${log_prefix_job_completed} $${GITHUB_JOB}"
rm -f $$V-jobs/$${GITHUB_RUN_ID}-$${GITHUB_JOB}-$$RUNNER_IDX.job
touch $$V-last-activity
EOFC

# check-runner-termination.sh is called periodically to check if the instance should terminate
cat > $$B/check-runner-termination.sh << EOFT
#!/bin/bash
exec >> /tmp/termination-check.log 2>&1
source $$B/runner-common.sh
V="/var/run/github-runner"
A="\\$$V-last-activity"
J="\\$$V-jobs"
H="\\$$V-has-run-job"
[ ! -f "\\$$A" ] && touch "\\$$A"
L=\\$$(stat -c %Y "\\$$A" 2>/dev/null || echo 0)
N=\\$$(date +%s)
I=\\$$((N-L))
[ -f "\\$$H" ] && G=\\$${RUNNER_GRACE_PERIOD:-60} || G=\\$${RUNNER_INITIAL_GRACE_PERIOD:-180}
R=\\$$(grep -l '\"status\":\"running\"' \\$$J/*.job 2>/dev/null | wc -l || echo 0)
if [ \\$$R -eq 0 ] && [ \\$$I -gt \\$$G ]; then
  log "TERMINATING: idle \\$$I > grace \\$$G"
  deregister_all_runners
  flush_cloudwatch_logs
  debug_sleep_and_shutdown
else
  [ \\$$R -gt 0 ] && log "\\$$R job(s) running" || log "Idle \\$$I/\\$$G sec"
fi
EOFT

chmod +x $$B/job-started-hook.sh $$B/job-completed-hook.sh $$B/check-runner-termination.sh

# Set up job tracking directory
mkdir -p $$V-jobs
touch $$V-last-activity

# Set up periodic termination check using systemd
cat > /etc/systemd/system/runner-termination-check.service << EOF
[Unit]
Description=Check GitHub runner termination conditions
After=network.target
[Service]
Type=oneshot
Environment="RUNNER_GRACE_PERIOD=$runner_grace_period"
Environment="RUNNER_INITIAL_GRACE_PERIOD=$runner_initial_grace_period"
ExecStart=$$B/check-runner-termination.sh
EOF

cat > /etc/systemd/system/runner-termination-check.timer << EOF
[Unit]
Description=Periodic GitHub runner termination check
Requires=runner-termination-check.service
[Timer]
OnBootSec=60s
OnUnitActiveSec=${runner_poll_interval}s
[Install]
WantedBy=timers.target
EOF

systemctl daemon-reload
systemctl enable runner-termination-check.timer
systemctl start runner-termination-check.timer

# Build metadata labels (these will be added to the runner labels)
METADATA_LABELS=",instance-id:$${INSTANCE_ID},instance-type:$${INSTANCE_TYPE}"
if [ -n "$github_workflow" ]; then
  WORKFLOW_LABEL=$$(echo "$github_workflow" | tr ' /' '-' | tr -cd '[:alnum:]-_')
  METADATA_LABELS="$${METADATA_LABELS},workflow:$${WORKFLOW_LABEL}"
fi
[ -n "$github_run_id" ] && METADATA_LABELS="$${METADATA_LABELS},run-id:$github_run_id"
[ -n "$github_run_number" ] && METADATA_LABELS="$${METADATA_LABELS},run-number:$github_run_number"

log "Setting up $$RUNNERS_PER_INSTANCE runner(s)"

# Export functions for subprocesses (variables already exported from runner-common.sh)
export -f configure_runner
export -f log
export -f log_error
export -f get_metadata
export -f flush_cloudwatch_logs
export -f deregister_all_runners
export -f debug_sleep_and_shutdown
export -f wait_for_dpkg_lock

# Parse space-delimited tokens and pipe-delimited labels
IFS=' ' read -ra tokens <<< "$runner_tokens"
IFS='|' read -ra labels <<< "$runner_labels"

num_runners=$${#tokens[@]}
log "Configuring $$num_runners runner(s) in parallel"

# Start configuration for each runner in parallel
pids=()
for i in $${!tokens[@]}; do
  token=$${tokens[$$i]}
  label=$${labels[$$i]:-}
  if [ -z "$$token" ]; then
    log_error "No token for runner $$i"
    continue
  fi
  (
    # Override ERR trap in subshell to prevent global side effects
    trap 'echo "Subshell error on line $$LINENO" >&2; exit 1' ERR
    configure_runner $$i "$$token" "$${label}$$METADATA_LABELS" "$$homedir" "$repo" "$$INSTANCE_ID" "$runner_grace_period" "$runner_initial_grace_period"
    echo $$? > /tmp/runner-$$i-status
  ) &
  pids+=($$!)
  log "Started configuration for runner $$i (PID: $${pids[-1]})"
done

# Wait for all background jobs to complete
log "Waiting for all runner configurations to complete..."
failed=0
succeeded=0
for i in $${!pids[@]}; do
  wait $${pids[$$i]}
  if [ -f /tmp/runner-$$i-status ]; then
    status=$$(cat /tmp/runner-$$i-status)
    rm -f /tmp/runner-$$i-status
    if [ "$$status" != "0" ]; then
      log_error "Runner $$i configuration failed"
      failed=$$((failed + 1))
    else
      succeeded=$$((succeeded + 1))
    fi
  fi
done

# Allow partial success - only terminate if ALL runners failed
if [ $$succeeded -eq 0 ] && [ $$failed -gt 0 ]; then
  terminate_instance "All runners failed to register"
elif [ $$failed -gt 0 ]; then
  log "WARNING: $$failed runner(s) failed, but $$succeeded succeeded. Continuing with partial capacity."
fi

if [ $$succeeded -gt 0 ]; then
  log "$$succeeded runner(s) registered and started successfully"
  touch $$V-registered
else
  log_error "No runners registered successfully"
fi

# Kill registration watchdog now that runners are registered
if [ -f $$V-watchdog.pid ]; then
  WATCHDOG_PID=$$(cat $$V-watchdog.pid)
  kill $$WATCHDOG_PID 2>/dev/null || true
  rm -f $$V-watchdog.pid
fi

# Final setup - ensure runner directories are accessible for debugging
touch $$V-started
chmod o+x $$homedir
for RUNNER_DIR in $$homedir/runner-*; do
  [ -d "$$RUNNER_DIR/_diag" ] && chmod 755 "$$RUNNER_DIR/_diag"
done
